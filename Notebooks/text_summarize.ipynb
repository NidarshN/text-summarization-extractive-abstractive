{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization on BBC News Corpus using Abstractive and Extractive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/nidarsh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/nidarsh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nidarsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# System Library\n",
    "import os\n",
    "\n",
    "# Data Wrangling Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "import textwrap\n",
    "from termcolor import colored\n",
    "\n",
    "# Machine Learning and NLP Libraries\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AdamW, T5ForConditionalGeneration, T5TokenizerFast as T5Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import gensim\n",
    "from rouge import rouge\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Graph Plotting Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "rcParams['figure.figsize']=16,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../Dataset/'\n",
    "DATA_PATH = '../Dataset/bbc-news-data.csv'\n",
    "DOCUMENTS_PATH = '../Documents/'\n",
    "CHECKPOINT_PATH = '../Checkpoints/'\n",
    "WORD2VEC_PATH = '../Checkpoints/bbc_word2vec.wordvectors_train.txt'\n",
    "MODEL_CHECKPOINT = './Checkpoints/model.ckpt'\n",
    "MODEL_NAME = 't5-base'\n",
    "EXTRACTIVE_EPOCHS = 3\n",
    "ABSTRACTIVE_EPOCHS = 3\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      category filename                              title  \\\n",
      "0     business  001.txt  Ad sales boost Time Warner profit   \n",
      "1     business  002.txt   Dollar gains on Greenspan speech   \n",
      "2     business  003.txt  Yukos unit buyer faces loan claim   \n",
      "3     business  004.txt  High fuel prices hit BA's profits   \n",
      "4     business  005.txt  Pernod takeover talk lifts Domecq   \n",
      "...        ...      ...                                ...   \n",
      "2220      tech  397.txt   BT program to beat dialler scams   \n",
      "2221      tech  398.txt    Spam e-mails tempt net shoppers   \n",
      "2222      tech  399.txt            Be careful how you code   \n",
      "2223      tech  400.txt    US cyber security chief resigns   \n",
      "2224      tech  401.txt   Losing yourself in online gaming   \n",
      "\n",
      "                                                content  \n",
      "0      Quarterly profits at US media giant TimeWarne...  \n",
      "1      The dollar has hit its highest level against ...  \n",
      "2      The owners of embattled Russian oil giant Yuk...  \n",
      "3      British Airways has blamed high fuel prices f...  \n",
      "4      Shares in UK drinks and food firm Allied Dome...  \n",
      "...                                                 ...  \n",
      "2220   BT is introducing two initiatives to help bea...  \n",
      "2221   Computer users across the world continue to i...  \n",
      "2222   A new European directive could put software w...  \n",
      "2223   The man making sure US computer networks are ...  \n",
      "2224   Online role playing games are time-consuming,...  \n",
      "\n",
      "[2225 rows x 4 columns]\n",
      "Training Data Size: (1780, 4)\n",
      "Testing Data Size: (445, 4)\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'r') as f:\n",
    "    df_header = f.readline().split()\n",
    "df = pd.read_csv(DATA_PATH, names=df_header, sep='\\t', skiprows=1)\n",
    "df['content'] = df['content'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "print(df)\n",
    "print(f\"Training Data Size: {train_df.shape}\")\n",
    "print(f\"Testing Data Size: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractive:\n",
    "    \"\"\"\n",
    "    Extractive: Class used to perform Extractive Text Summarization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame = None,\n",
    "        test_df: pd.DataFrame = None,\n",
    "        model: Word2Vec = None,\n",
    "        word_lemmatizer: WordNetLemmatizer = None,\n",
    "        word2vec_path: str = WORD2VEC_PATH,\n",
    "        epochs: int = 3\n",
    "    ):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.model = model\n",
    "        self.word_lemmatizer = word_lemmatizer\n",
    "        self.word2vec_path = word2vec_path\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        \n",
    "    def word_vector_tokenization(self):\n",
    "        self.sentences = []\n",
    "        for i in tqdm(range(self.train_df.shape[0]), desc=\"Loading...\"):\n",
    "            content = self.train_df.loc[self.train_df.index[i]]['content']\n",
    "            tokens = nltk.sent_tokenize(content)\n",
    "            for token in tokens:\n",
    "                self.sentences.append(token.split(' '))\n",
    "        print('Storing Word Vectors!')\n",
    "        self.model = Word2Vec(self.sentences, min_count=1)\n",
    "        word_vectors = self.model.wv\n",
    "        word_vectors.save_word2vec_format(self.word2vec_path, binary=False)\n",
    "        print(f\"Word Vectors Saved at {self.word2vec_path}!\")\n",
    "\n",
    "    def extract_word_vectors(self) -> dict:\n",
    "        \"\"\"\n",
    "        Extracting word embeddings from wordvector file.\n",
    "        \"\"\"\n",
    "        word_embeddings = {}\n",
    "        f = open(self.word2vec_path, encoding='utf-8')\n",
    "        for index, line in enumerate(f):\n",
    "            if(index == 0):\n",
    "                continue\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            word_embeddings[word] = coefs\n",
    "\n",
    "        f.close()\n",
    "        return word_embeddings\n",
    "\n",
    "    def text_preprocessing(self, sentences: list) -> list:\n",
    "        \"\"\"\n",
    "        Pre processing text to remove unnecessary words.\n",
    "        \"\"\"\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        clean_words = None\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent)\n",
    "            words = [self.word_lemmatizer.lemmatize(word.lower()) for word in words if word.isalnum()]\n",
    "            clean_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        return clean_words\n",
    "\n",
    "    def sentence_vector_representation(self, sentences: list, word_embeddings: dict) -> list:\n",
    "        \"\"\"\n",
    "        Creating sentence vectors from word embeddings.\n",
    "        \"\"\"\n",
    "        sentence_vectors = []\n",
    "        for sent in sentences:\n",
    "            clean_words = self.text_preprocessing([sent])\n",
    "            # Averaging the sum of word embeddings of the sentence to get sentence embedding vector\n",
    "            v = sum([word_embeddings.get(word, np.zeros(100, )) for word in clean_words]) / (len(clean_words) + 0.001)\n",
    "            sentence_vectors.append(v)\n",
    "\n",
    "        return sentence_vectors\n",
    "\n",
    "    def create_similarity_matrix(self, sentences: list, sentence_vectors: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Using cosine similarity, generate similarity matrix.\n",
    "        \"\"\"\n",
    "        # Defining a zero matrix of dimension n * n\n",
    "        sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    # Replacing array value with similarity value.\n",
    "                    # Not replacing the diagonal values because it represents similarity with its own sentence.\n",
    "                    if(type(sentence_vectors[i]) != type(0.1) and type(sentence_vectors[j])!= type(0.1)):\n",
    "                        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1, 100), sentence_vectors[j].reshape(1, 100))[0, 0]\n",
    "\n",
    "        return sim_mat\n",
    "\n",
    "    def determine_sentence_rank(self, sentences: list, sim_mat: np.ndarray):\n",
    "        \"\"\"\n",
    "        Determining sentence rank using Page Rank algorithm.\n",
    "        \"\"\"\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        ranked_sentences = sorted([(scores[i], s[:15]) for i, s in enumerate(sentences)], reverse=True)\n",
    "        return ranked_sentences\n",
    "\n",
    "    def generate_summary(self, sentences: list, ranked_sentences: list):\n",
    "        \"\"\"\n",
    "        Generate a sentence for sentence score greater than average.\n",
    "        \"\"\"\n",
    "        # Get top 1/3 th ranked sentences\n",
    "        top_ranked_sentences = ranked_sentences[:int(len(sentences) / 3)] if len(sentences) >= 3 else ranked_sentences\n",
    "        sentence_count = 0\n",
    "        summary = ''\n",
    "\n",
    "        for i in sentences:\n",
    "            for j in top_ranked_sentences:\n",
    "                if i[:15] == j[1]:\n",
    "                    summary += i + ' '\n",
    "                    sentence_count += 1\n",
    "                    break\n",
    "        return summary\n",
    "\n",
    "    def train(self):\n",
    "        self.word_vector_tokenization()\n",
    "\n",
    "    def rouge_evaluation(self, evaluator, reference, hypothesis):\n",
    "        rouge_scores = evaluator.get_scores(hypothesis, reference)\n",
    "        rouge_df = pd.DataFrame(rouge_scores)\n",
    "        f1_val, precision_val, recall_val = rouge_df.mean(axis=1)\n",
    "        return [f1_val, precision_val, recall_val]\n",
    "\n",
    "    def bleu_evaluation(self, chencherry, reference, hypothesis):\n",
    "        bleu_score = sentence_bleu(reference, word_tokenize(hypothesis), smoothing_function=chencherry.method4, weights=(0.2, 0.4, 0.3, 0.1))\n",
    "        return bleu_score\n",
    "        \n",
    "\n",
    "    def test(self):\n",
    "        summarized_df = pd.DataFrame(columns=['original', 'summarized'], index=[i for i in range(test_df.shape[0])])\n",
    "        rouge_test_score = pd.DataFrame(columns=['f1_score', 'precision', 'recall'], index=[i for i in range(test_df.shape[0])])\n",
    "        evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=1,\n",
    "                        apply_best=0,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)\n",
    "        chencherry = SmoothingFunction()\n",
    "        bleu_scores_list = []\n",
    "        \n",
    "        for i in tqdm(range(test_df.shape[0]), desc=\"Loading...\"):\n",
    "            index = test_df.index[i]\n",
    "            test_content = test_df.loc[index]['content']\n",
    "            test_sentences = sent_tokenize(test_content)\n",
    "            word_embeddings = self.extract_word_vectors()\n",
    "            sentence_vectors = self.sentence_vector_representation(sentences=test_sentences, word_embeddings=word_embeddings)\n",
    "            similarity_mat = self.create_similarity_matrix(sentences=test_sentences, sentence_vectors=sentence_vectors)\n",
    "            ranked_sentences = self.determine_sentence_rank(sentences=test_sentences, sim_mat=similarity_mat)\n",
    "            summary = self.generate_summary(sentences=test_sentences, ranked_sentences=ranked_sentences)\n",
    "            summarized_df.iloc[i] = [test_content, summary]\n",
    "            rouge_test_score.iloc[i] = self.rouge_evaluation(evaluator=evaluator, reference=test_content, hypothesis=summary)\n",
    "            bleu_scores = self.bleu_evaluation(chencherry=chencherry, reference=test_sentences, hypothesis=summary)\n",
    "            bleu_scores_list.append(bleu_scores)\n",
    "        \n",
    "        print()\n",
    "        print('-' * 25 + 'BLEU SCORE' + '-' * 25)\n",
    "        print(f\"Score: {bleu_scores}\")\n",
    "        print('-' * 30 + '*' + '-' * 30) \n",
    "        print()\n",
    "        print('-' * 25 + 'ROUGE SCORE' + '-' * 25)\n",
    "        print(rouge_test_score.mean())\n",
    "        print('-' * 30 + '*' + '-' * 30)\n",
    "\n",
    "        print()\n",
    "        print('-' * 25 + 'Saving original text and its generated summary' + '-' * 25)\n",
    "        summarized_df.to_csv(DOCUMENTS_PATH + 'Extractive_Summarization_Result.csv')\n",
    "\n",
    "    def test_custom(self, custom_text: str = 'Dummy Text'):\n",
    "        test_content = custom_text\n",
    "        test_sentences = sent_tokenize(test_content)\n",
    "        word_embeddings = self.extract_word_vectors()\n",
    "        sentence_vectors = self.sentence_vector_representation(sentences=test_sentences, word_embeddings=word_embeddings)\n",
    "        similarity_mat = self.create_similarity_matrix(sentences=test_sentences, sentence_vectors=sentence_vectors)\n",
    "        ranked_sentences = self.determine_sentence_rank(sentences=test_sentences, sim_mat=similarity_mat)\n",
    "        summary_custom = self.generate_summary(sentences=test_sentences, ranked_sentences=ranked_sentences)\n",
    "        print()\n",
    "        print('-' * 25 + 'YOUR TEXT' + '-' * 25)\n",
    "        print(test_content)\n",
    "        print('-' * 30 + '*' + '-' * 30)\n",
    "        print('-' * 25 + 'EXTRACTIVE SUMMARY' + '-' * 25)\n",
    "        print(summary_custom)\n",
    "        print('-' * 30 + '*' + '-' * 30) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_obj = Extractive(\n",
    "                        train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        word_lemmatizer=WordNetLemmatizer(),\n",
    "                        word2vec_path=WORD2VEC_PATH,\n",
    "                        epochs=EXTRACTIVE_EPOCHS\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2725340a342c48e69f208f91792345a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading...:   0%|          | 0/1780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing Word Vectors!\n",
      "Word Vectors Saved at ../Checkpoints/bbc_word2vec.wordvectors_train.txt!\n"
     ]
    }
   ],
   "source": [
    "extract_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e4d8a01a1543358714a3a80de703f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading...:   0%|          | 0/445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------BLEU SCORE-------------------------\n",
      "Score: 0.011549270572159\n",
      "------------------------------*------------------------------\n",
      "\n",
      "-------------------------ROUGE SCORE-------------------------\n",
      "f1_score     0.378655\n",
      "precision    0.455189\n",
      "recall       0.344460\n",
      "dtype: float64\n",
      "------------------------------*------------------------------\n",
      "\n",
      "-------------------------Saving original text and its generated summary-------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_obj.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------YOUR TEXT-------------------------\n",
      "\n",
      "The wave roared towards them with speed and violence they had not anticipated. They both turned to run but by that time it was too late. The wave crashed into their legs sweeping both of them off of their feet. They now found themselves in a washing machine of saltwater, getting tumbled and not know what was up or down. Both were scared, not knowing how this was going to end, but it was by far the best time of the trip thus far. Welcome to my world. You will be greeted by the unexpected here and your mind will be challenged and expanded in ways that you never thought possible. That is if you are able to survive... It really shouldn't have mattered to Betty. That's what she kept trying to convince herself even if she knew it mattered to Betty more than practically anything else. Why was she trying to convince herself otherwise? As she stepped forward to knock on Betty's door, she still didn't have a convincing answer to this question that she'd been asking herself for more than two years now. The headphones were on. They had been utilized on purpose. She could hear her mom yelling in the background, but couldn't make out exactly what the yelling was about. That was exactly why she had put them on. She knew her mom would enter her room at any minute, and she could pretend that she hadn't heard any of the previous yelling. It was their first date and she had been looking forward to it the entire week. She had her eyes on him for months, and it had taken a convoluted scheme with several friends to make it happen, but he'd finally taken the hint and asked her out. After all the time and effort she'd invested into it, she never thought that it would be anything but wonderful. It goes without saying that things didn't work out quite as she expected.\n",
      "\n",
      "------------------------------*------------------------------\n",
      "-------------------------EXTRACTIVE SUMMARY-------------------------\n",
      "They now found themselves in a washing machine of saltwater, getting tumbled and not know what was up or down. Both were scared, not knowing how this was going to end, but it was by far the best time of the trip thus far. You will be greeted by the unexpected here and your mind will be challenged and expanded in ways that you never thought possible. That's what she kept trying to convince herself even if she knew it mattered to Betty more than practically anything else. Why was she trying to convince herself otherwise? She had her eyes on him for months, and it had taken a convoluted scheme with several friends to make it happen, but he'd finally taken the hint and asked her out. It goes without saying that things didn't work out quite as she expected. \n",
      "------------------------------*------------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_obj.test_custom(custom_text=\"\"\"\n",
    "The wave roared towards them with speed and violence they had not anticipated. They both turned to run but by that time it was too late. The wave crashed into their legs sweeping both of them off of their feet. They now found themselves in a washing machine of saltwater, getting tumbled and not know what was up or down. Both were scared, not knowing how this was going to end, but it was by far the best time of the trip thus far. Welcome to my world. You will be greeted by the unexpected here and your mind will be challenged and expanded in ways that you never thought possible. That is if you are able to survive... It really shouldn't have mattered to Betty. That's what she kept trying to convince herself even if she knew it mattered to Betty more than practically anything else. Why was she trying to convince herself otherwise? As she stepped forward to knock on Betty's door, she still didn't have a convincing answer to this question that she'd been asking herself for more than two years now. The headphones were on. They had been utilized on purpose. She could hear her mom yelling in the background, but couldn't make out exactly what the yelling was about. That was exactly why she had put them on. She knew her mom would enter her room at any minute, and she could pretend that she hadn't heard any of the previous yelling. It was their first date and she had been looking forward to it the entire week. She had her eyes on him for months, and it had taken a convoluted scheme with several friends to make it happen, but he'd finally taken the hint and asked her out. After all the time and effort she'd invested into it, she never thought that it would be anything but wonderful. It goes without saying that things didn't work out quite as she expected.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summary_moduler import SummaryDataset, SummaryModel, SummaryDataModule\n",
    "\n",
    "class Abstractive:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame = None,\n",
    "        test_df: pd.DataFrame = None,\n",
    "        model: SummaryModel = None,\n",
    "        tokenizer: T5Tokenizer = None,\n",
    "        trainer: pl.Trainer = None,\n",
    "        datamodule: SummaryDataModule = None\n",
    "\n",
    "    ):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.trainer = trainer\n",
    "        self.datamodule = datamodule\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        title_token_counts, content_token_counts = [], []\n",
    "        for _, row in self.train_df.iterrows():\n",
    "            title_token_count = len(self.tokenizer.encode(row['title']))\n",
    "            title_token_counts.append(title_token_count)\n",
    "\n",
    "            content_token_count = len(self.tokenizer.encode(row['content']))\n",
    "            content_token_counts.append(content_token_count)\n",
    "        \n",
    "        self.data_module = SummaryDataModule(train_df=self.train_df, test_df=self.test_df, tokenizer=self.tokenizer)\n",
    "        \n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath='Checkpoints',\n",
    "            filename='model',\n",
    "            save_top_k=1,\n",
    "            verbose=True,\n",
    "            monitor='validation_loss',\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        logger = TensorBoardLogger(\"lightning_logs\", name='summary')\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "                logger=logger,\n",
    "                callbacks=[checkpoint_callback, TQDMProgressBar(refresh_rate=10)],\n",
    "                max_epochs=ABSTRACTIVE_EPOCHS,\n",
    "                devices=2,\n",
    "                accelerator=\"auto\",\n",
    "                log_every_n_steps=1,\n",
    "                enable_model_summary=True,\n",
    "                enable_progress_bar=True\n",
    "            )\n",
    "        trainer.fit(model=self.model, train_dataloaders=self.data_module.train_dataloader, val_dataloaders=self.data_module.validation_dataloader)\n",
    "        self.trainer = trainer\n",
    "        self.trainer.save_checkpoint(MODEL_CHECKPOINT)\n",
    "\n",
    "    def test(self):\n",
    "        self.trainer.test(dataloaders=self.data_module.test_dataloader)\n",
    "\n",
    "\n",
    "    def summarizeText(self, eval_flag='false', text='Dummy Data'):\n",
    "        trained_model = SummaryModel.load_from_checkpoint(MODEL_CHECKPOINT)\n",
    "        trained_model.freeze()\n",
    "        text_encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length = 512,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            add_special_tokens = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        generated_ids = trained_model.model.generate(\n",
    "            input_ids = text_encoding['input_ids'],\n",
    "            attention_mask = text_encoding['attention_mask'],\n",
    "            max_length = 150,\n",
    "            num_beams = 2,\n",
    "            repetition_penalty = 2.5,\n",
    "            length_penalty = 2.0,\n",
    "            early_stopping = True\n",
    "            \n",
    "        )\n",
    "\n",
    "        preds = [\n",
    "            self.tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            for generated_id in generated_ids\n",
    "        ]\n",
    "        abstractive_summary = \"\".join(preds)\n",
    "        if(eval_flag):\n",
    "            return abstractive_summary\n",
    "        \n",
    "        print()\n",
    "        print('-' * 25 + 'YOUR TEXT' + '-' * 25)\n",
    "        print(text)\n",
    "        print('-' * 30 + '*' + '-' * 30)\n",
    "        print('-' * 25 + 'ABSTRACTIVE SUMMARY' + '-' * 25)\n",
    "        print(abstractive_summary)\n",
    "        print('-' * 30 + '*' + '-' * 30) \n",
    "\n",
    "    def rouge_evaluation(self):\n",
    "        abstractive_summarized_df = pd.DataFrame(columns=['original', 'summarized'], index=[i for i in range(self.test_df.shape[0])])\n",
    "        abstractive_rouge_test_score = pd.DataFrame(columns=['f1_score', 'precision', 'recall'], index=[i for i in range(self.test_df.shape[0])])\n",
    "        evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=1,\n",
    "                        apply_best=0,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)\n",
    "\n",
    "        for i in tqdm(range(self.test_df.shape[0]), desc=\"Loading...\"):\n",
    "            index = self.test_df.index[i]\n",
    "            test_content = test_df.loc[index]['content']\n",
    "            summary = self.summarizeText(eval_flag=True, text=test_content)\n",
    "            abstractive_summarized_df.iloc[i] = [test_content, summary]\n",
    "            rouge_score = evaluator.get_scores(summary, test_content)\n",
    "            rouge_df = pd.DataFrame(rouge_score)\n",
    "            f1_val, precision_val, recall_val = rouge_df.mean(axis=1)\n",
    "            abstractive_rouge_test_score.iloc[i] = [f1_val, precision_val, recall_val]\n",
    "        \n",
    "        print()\n",
    "        print('-' * 25 + 'ROUGE SCORE' + '-' * 25)\n",
    "        print(abstractive_rouge_test_score.mean())\n",
    "        print('-' * 30 + '*' + '-' * 30)\n",
    "\n",
    "        print()\n",
    "        print('-' * 25 + 'Saving original text and its generated summary' + '-' * 25)\n",
    "        abstractive_summarized_df.to_csv(DOCUMENTS_PATH + 'Abstractive_Summarization_Result.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nidarsh/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "abstractive = Abstractive(\n",
    "                train_df=train_df,\n",
    "                test_df=test_df,\n",
    "                model= SummaryModel(),\n",
    "                tokenizer= T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractive.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e924b46b9fad4af6a0230aa9eead8ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading...:   0%|          | 0/445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------ROUGE SCORE-------------------------\n",
      "f1_score     0.134473\n",
      "precision    0.146104\n",
      "recall       0.128985\n",
      "dtype: float64\n",
      "------------------------------*------------------------------\n",
      "\n",
      "-------------------------Saving original text and its generated summary-------------------------\n"
     ]
    }
   ],
   "source": [
    "abstractive.rouge_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractive.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------YOUR TEXT-------------------------\n",
      "\n",
      "The wave roared towards them with speed and violence they had not anticipated. They both turned to run but by that time it was too late. The wave crashed into their legs sweeping both of them off of their feet. They now found themselves in a washing machine of saltwater, getting tumbled and not know what was up or down. Both were scared, not knowing how this was going to end, but it was by far the best time of the trip thus far. Welcome to my world. You will be greeted by the unexpected here and your mind will be challenged and expanded in ways that you never thought possible. That is if you are able to survive... It really shouldn't have mattered to Betty. That's what she kept trying to convince herself even if she knew it mattered to Betty more than practically anything else. Why was she trying to convince herself otherwise? As she stepped forward to knock on Betty's door, she still didn't have a convincing answer to this question that she'd been asking herself for more than two years now. The headphones were on. They had been utilized on purpose. She could hear her mom yelling in the background, but couldn't make out exactly what the yelling was about. That was exactly why she had put them on. She knew her mom would enter her room at any minute, and she could pretend that she hadn't heard any of the previous yelling. It was their first date and she had been looking forward to it the entire week. She had her eyes on him for months, and it had taken a convoluted scheme with several friends to make it happen, but he'd finally taken the hint and asked her out. After all the time and effort she'd invested into it, she never thought that it would be anything but wonderful. It goes without saying that things didn't work out quite as she expected.\n",
      "\n",
      "------------------------------*------------------------------\n",
      "-------------------------ABSTRACTIVE SUMMARY-------------------------\n",
      "A group of young people who had been chasing each other for years have decided they will not be allowed to run away. They were all over the place at the time, but it was only after a series of events that led them to decide whether to stay or not to return. The group's leader, John McConnell, said: \"I don't think I would ever want to go back to my old ways.\" He told BBC Radio Five Live: \"I didn't know what to expect from me when I came up with the idea. It doesn't matter how much money is spent on something else,\n",
      "------------------------------*------------------------------\n"
     ]
    }
   ],
   "source": [
    "abstractive.summarizeText(text=\"\"\"\n",
    "The wave roared towards them with speed and violence they had not anticipated. They both turned to run but by that time it was too late. The wave crashed into their legs sweeping both of them off of their feet. They now found themselves in a washing machine of saltwater, getting tumbled and not know what was up or down. Both were scared, not knowing how this was going to end, but it was by far the best time of the trip thus far. Welcome to my world. You will be greeted by the unexpected here and your mind will be challenged and expanded in ways that you never thought possible. That is if you are able to survive... It really shouldn't have mattered to Betty. That's what she kept trying to convince herself even if she knew it mattered to Betty more than practically anything else. Why was she trying to convince herself otherwise? As she stepped forward to knock on Betty's door, she still didn't have a convincing answer to this question that she'd been asking herself for more than two years now. The headphones were on. They had been utilized on purpose. She could hear her mom yelling in the background, but couldn't make out exactly what the yelling was about. That was exactly why she had put them on. She knew her mom would enter her room at any minute, and she could pretend that she hadn't heard any of the previous yelling. It was their first date and she had been looking forward to it the entire week. She had her eyes on him for months, and it had taken a convoluted scheme with several friends to make it happen, but he'd finally taken the hint and asked her out. After all the time and effort she'd invested into it, she never thought that it would be anything but wonderful. It goes without saying that things didn't work out quite as she expected.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea195e53f2ba984fe8d7c4b3c3a4b8462161715ad1ec800885ad4718db60b3a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
